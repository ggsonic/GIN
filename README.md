[UPDATE -2022-09-02] our new paper is published : "FaithNet: A Generative Framework in Human Mentalizing"[[here](http://dx.doi.org/10.13140/RG.2.2.22140.21121) or [here](https://doi.org/10.36227/techrxiv.21359667.v1)]

# GIN
Official repository for the work: "Preliminary concept of General intelligent network (GIN) for brain-like intelligence"[[pdf](https://vixra.org/pdf/2201.0188v1.pdf)]

# Some Explanation

* HPP  Why we name it HPP? Because HPP is different from embeddings. As for HPP, The basic element is **H**idden state or node (**P**oint) with acting edges or branches (**P**atch),with each branch with a particular context. Deep Learning Models can learn individual embeddings, but HPP is  learned from contextual related environment.

* NKC  The process of Neural Knowledge Compression do similar thing like encoding HPPs. With input horizontal sequence-to-sequence HPPs, NKC got output one HPP with branch-to-branch forward graph, a.k.a a new HPP graph. Imagine a scenario, we have just watched a movie, which is one hour MP4 video file with 25 fps; Then After NKC, our brain or GIN output one HPP with dozens of forward branches,which may be only hundreds of bytes! Later on When we need to recall this movie, this HPP graph would be parsed with our GAN-like GIN Generative Network. NKC keep running when people are sleeping or dreaming, with brain's memory space freed up and got deeper knowledge and wisdom.

* GIN  GIN is based on HPP dynamics and Autonomous NKC, it is totally different from modern Deep Learning models.

* CFR  In our paper, Deep Learning optimization process is essentially a variant of counterfactual regret minimization (CFR). This concept was first proposed so far as we know. As presented in our paper, in inner information sets, each inner state node does not know which of the node states it is in. Self-Attention or MLP are alignment of different strategies. We will release code about this.
